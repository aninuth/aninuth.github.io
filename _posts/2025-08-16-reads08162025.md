---
title: "Things I've been reading - week of 08/16"
date: 2025-08-16
permalink: /posts/2025/08/reads08162025/
tags:
  - reads
  - prediction models
  - LLMs
  - Polygenics
  - Psychiatry
---

Here are some things I read/enjoyed this week - please reach out to me if you have thoughts on any of these or have answers to my questions:

**Papers**

[When accurate prediction models yield harmful self-
fulfilling prophecies](https://doi.org/10.1016/j.patter.2025.101229)

Fascinating consideration of the second-order implications of using prediction models to determine treatment decisions for patients. This is something I never really considered, but it is critical considering how much hype there is behind using models to predict patient outcomes, a key premise of the "individualized/precision medicine" ecosystem. 

The authors give a hypothetical that perfectly illustrates both the harmful and self-fulfilling parts of this issue. The formal version is linked [here] (https://ars.els-cdn.com/content/image/1-s2.0-S2666389925000777-mmc1.pdf), but a summary:

The treatment/intervention is palliative radiotherapy for end-stage cancer patients.  Due to the side effects and nature of the treatment, it is advised that treatment is not over-administered. As such, the medical center gives the therapy to patients with the longest expected overall survival, based on the assumption that the treatment is most beneficial for these patients (a logical assumption). In order to come up with an EV for survival, the team builds a prediction model to predict the probability of 6-month overall survival based on tumor growth data, among other factors.

This leads to the clinic only administering the treatment to patients with slow-growing tumors (since fast-growing tumors are more aggressive/these patients have a shorter surival on aggregate). However, it is well-known (according to the paper, this is news to me!) that fast-growing tumors respond better to radiotherapy than slow-growing tumors. 

This leads to the central paradox of the paper: This model has good discrimination before and after deployment. Ceasing radiotherapy for patients with fast-growing tumors will worsen their outcomes, leading to higher AUC post-deployment. However, this system is harmful for everyone -- those who not really benefit from the treatment get it (and are subject to the discomfort and side effects), while those who would benefit from the treatment are not given radiotherapy. Thus, we end up in a self-fulfilling situation without a clear way out.

As someone who had read my share of research papers (and conducted my share of experiments) in which AUC is all that matters, this paper is pretty eye-opening when thinking about how to evaluate model performance post-deployment. The authors suggest looking at the pre-existing treatment policy and $\Delta AUC$ in combination as a way to avoid this trap. More broadly, I think the paper makes a great case for seriously thinking about how we measure the impact of integrating ML into a pre-existing system. 




[A Case of Bromism Inﬂuenced by Use of Artiﬁcial Intelligence](https://www.acpjournals.org/doi/epdf/10.7326/aimcc.2024.1260)

To my knowledge, the first case report in which a patient has needed medical attention due to following advice from an LLM. It's easy to find this case a little silly - a 60 year old man, "inspired by his history of studying nutrition in college", tried to eliminate chloride from his diet. Supposedly, this was due to reading about the negative effects of Sodium Chloride (salt, for the non-chemists in the room) but only finding suggestions for reducing sodium intake. What a gap in the literature!

While his exact conversation with ChatGPT is not made available to the researchers (or to readers), it seems that the subject replaced his sodium chloride with sodium bromide (acquired from the internet) after consulting ChatGPT, which mentioned that bromide can often be swapped with chloride.

It seems reductive to put the blame entirely on LLMs or the end user here -- medical literature is quite challenging to work through, and a system that promises to digest it and provide readable, usable, answers is quite tempting. This is not a particularly unreasonable use case of an LLM, and I don't think the engineers behind these models can wipe their hands clean of every consequence of model output.

On the other hand, it seems like the user did not specify the context at all - "I'm trying to reduce my salt intake for my health. What would you recommend I replace sodium chloride with?" would yield radically different reponses than "what are some chloride substitutes?". While models should police their output better for things that are health-related, it would be impossible to ascertain what the user's objective was in cases like these. 

A real challenge that I don't see an easy solution for.

This is also a sobering reminder of the dangers of attempting to be an epistemic superhero. I consider myself well-informed and frequently "do the research", but I have almost certainly made mistakes similar in scope to not realizing that salt is colloquially referred to as chloride. I routinely catch these as I learn more/dig in further, but who knows how many misconceptions I've acquired that I have yet to realize are wrong?



**Substacks/Blogs**

[The biggest mystery in neuroscience, according to me](https://ccli.substack.com/p/the-biggest-mystery-in-neuroscience)
This is one of my favorite Substacks as of late, and this series in particular has taught me a lot about the current state of cognitive neuroscience. The lack of progress in this space really tempers the expectations of a lot of AI truthers that we will be able to simulate human brains in our [lifetime](https://www.smbc-comics.com/index.php?db=comics&id=1968).

[Claude Opus 4 and 4.1 can now end a rare subset of conversations](https://www.anthropic.com/research/end-subset-conversations)
Once again, Anthropic stepping up as one of the pioneers of AI transparency and safety. I have my reservations about the moral status of LLMs and AI agents, but I'm absolutely in support of small interventions like these that might be able to prevent suffering. From a qualitatively utilitarian perspective, I also can't imagine that the sort of pleasure these conversations (abusing LLMs) provide the end-user is one that we want to encourage. 

[Q&A with Pim Welle on the New York Fed Staff Report on Involuntary Hospitalization ](https://www.psychiatrymargins.com/p/q-and-a-with-pim-welle-on-the-new?source=queue)
This is another Substack I've really learned a lot from, and almost every post belonds on here. I'm selecting this one in particular due to the intersection of data work and the usual qualitative research I'm used to seeing in this space. This isn't something I think about often, but I'm glad that people take the implications and consequences of involuntary holds seriously, and are constantly working on testing their predictions and updating their models. Whether that leads to actual meaningful reform, of course, is a different story - but this was a fascinating read.

[What We Talk About When We Talk About Risk](https://theinfinitesimal.substack.com/p/what-we-talk-about-when-we-talk-about)
Surprise! Another Substack that I think is exceptional. This is an honest take about the reality of polygenic risk prediction, particularly for embryo selection. There's a lot of hype in these spaces, and not without reason, but the author (a statistical geneticist) does an incredible job explaining the fallacies that people like Scott Alexander fall for when considering new technologies like these.